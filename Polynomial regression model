To build a polynomial regression model for predicting employee salaries, we'll use a dataset where features such as years of experience, age, or education level might influence the salary. For this example, we'll assume we have a dataset with a feature like years of experience as the independent variable and salary as the dependent variable.

Polynomial regression is an extension of linear regression where we use polynomial features of the independent variable (e.g., experience, age) to fit the data.

Steps:
Load and prepare the dataset.
Preprocess the data and transform features to polynomial features.
Train a polynomial regression model.
Evaluate the model.
Predict salary for new input.
Python Code Example
Here, we'll use a hypothetical dataset for employee salaries and implement polynomial regression using scikit-learn.

python
Copy code
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error

# Hypothetical dataset: Years of experience vs. Salary
# Replace this with your actual dataset
data = {
    'YearsExperience': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Salary': [40000, 45000, 50000, 60000, 65000, 70000, 75000, 80000, 85000, 90000]
}

# Convert the data to a pandas DataFrame
import pandas as pd
df = pd.DataFrame(data)

# Split the data into features (X) and target (y)
X = df[['YearsExperience']].values  # Independent variable (Years of experience)
y = df['Salary'].values  # Dependent variable (Salary)

# Step 1: Polynomial Feature Transformation (degree 2, you can change this)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# Step 2: Train-test split (to evaluate the model)
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.3, random_state=42)

# Step 3: Train the polynomial regression model
polynomial_regressor = LinearRegression()
polynomial_regressor.fit(X_train, y_train)

# Step 4: Predict on the test set
y_pred = polynomial_regressor.predict(X_test)

# Step 5: Evaluate the model (using mean squared error)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# Step 6: Plot the results (visualization)
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', label='Actual Salary')  # Actual data points
plt.plot(X, polynomial_regressor.predict(poly.transform(X)), color='red', label='Polynomial Regression Line')  # Prediction line
plt.title('Polynomial Regression: Years of Experience vs Salary')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.legend()
plt.show()

# Step 7: Predict salary for a new sample (e.g., 6.5 years of experience)
new_sample = np.array([[6.5]])  # New experience value
new_sample_poly = poly.transform(new_sample)
predicted_salary = polynomial_regressor.predict(new_sample_poly)
print(f"Predicted Salary for 6.5 years of experience: ${predicted_salary[0]:,.2f}")
Explanation of the Code
Dataset:

We create a small dataset with years of experience (YearsExperience) and corresponding salaries (Salary).
You can replace the data dictionary with your own dataset.
Polynomial Feature Transformation:

We transform the independent variable (Years of Experience) into polynomial features using PolynomialFeatures from sklearn.preprocessing.
This allows us to fit a polynomial regression curve instead of a linear line.
Train-Test Split:

We split the data into a training set (70%) and a testing set (30%) to evaluate how well the model generalizes.
Model Training:

We train a linear regression model (LinearRegression), but since we transformed the features to be polynomial, this effectively becomes polynomial regression.
Model Evaluation:

We use Mean Squared Error (MSE) to evaluate the accuracy of the model. You can also use other metrics such as RÂ² score to understand how well the model fits the data.
Visualization:

We plot the actual data points and the polynomial regression curve to visually check how well the model fits the data.
Prediction:

We predict the salary for a new sample with 6.5 years of experience and output the predicted salary.
Understanding Polynomial Regression
Polynomial Regression is useful when the relationship between variables is not strictly linear. In this example, the data may show a curving trend, which polynomial regression can capture.
The degree of the polynomial (e.g., degree=2 for a quadratic function) controls how flexible the model is. Higher-degree polynomials can fit more complex curves, but they can also lead to overfitting.
Next Steps
Hyperparameter Tuning: Try different degrees for the polynomial features and evaluate which degree gives the best model (lowest error) using cross-validation.
Visualize Results: More complex datasets might benefit from visualizations in 3D (if there are multiple features), or you can plot residuals to see how well the model fits.
